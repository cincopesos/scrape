# Web Scraper

A high-performance parallel web scraper that converts websites into organized Markdown files. Built with Python and `crawl4ai`, `requests`, `psutil`, `rich`, and `asyncio`, this tool efficiently processes websites by leveraging their sitemaps and supports concurrent scraping with built-in memory monitoring.

## Features

- ğŸš€ Parallel scraping with configurable concurrency
- ğŸ“‘ Automatic sitemap detection and processing
- ğŸ“ Organized output with clean directory structure
- ğŸ’¾ Memory-efficient with built-in monitoring
- ğŸŒ Browser-based scraping using crawl4ai
- ğŸ“Š Progress tracking and detailed logging
- ğŸ” Preview mode with dry-run option

## Requirements

- Python 3.7+
- crawl4ai
- rich
- psutil
- requests

## Installation

1. Clone the repository:

```bash
git clone https://github.com/rkabrick/scrape.git
cd web-scraper
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

## Usage

Basic usage:

```bash
python scrape https://example.com
```

### Command Line Options

```bash
scrape [-h] [--max-concurrent MAX_CONCURRENT] [-v] [--dry-run] url
```

Arguments:

- `url`: The target URL to scrape (must include http:// or https://)
- `--max-concurrent`: Maximum number of concurrent scrapers (default: 3)
- `-v`: Increase verbosity level
  - `-v`: Show file names
  - `-vv`: Show browser output
  - `-vvv`: Show memory monitoring
- `--dry-run`: Preview the file structure without performing the scrape

### Examples

1. Basic scraping:

```bash
scrape https://example.com
```

2. Scraping with increased concurrency:

```bash
scrape --max-concurrent 5 https://example.com
```

3. Preview mode with file structure:

```bash
scrape --dry-run https://example.com
```

4. Verbose output with memory monitoring:

```bash
scrape -vvv https://example.com
```

## Output Structure

The scraper creates an organized directory structure based on the website's URL paths. For example:

```
example.com/
â”œâ”€â”€ index.md
â”œâ”€â”€ about/
â”‚   â””â”€â”€ index.md
â”œâ”€â”€ blog/
â”‚   â”œâ”€â”€ post1.md
â”‚   â””â”€â”€ post2.md
â””â”€â”€ products/
    â”œâ”€â”€ category1/
    â”‚   â””â”€â”€ item1.md
    â””â”€â”€ category2/
        â””â”€â”€ item2.md
```

## Features in Detail

### Sitemap Processing

- Automatically detects and processes XML sitemaps
- Falls back to single URL processing if no sitemap is found
- Supports both simple and nested sitemap structures

### Memory Management

- Built-in memory monitoring for resource-intensive operations
- Configurable concurrent scraping to balance performance and resource usage
- Automatic cleanup of browser instances

### File Organization

- Intelligent path handling and file naming
- Duplicate file name resolution
- Clean, SEO-friendly file structure
- Markdown output for compatibility

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Built with [crawl4ai](https://github.com/example/crawl4ai) for reliable web scraping
- Uses [rich](https://github.com/Textualize/rich) for beautiful terminal output
- Memory monitoring powered by [psutil](https://github.com/giampaolo/psutil)

## Support

For issues, questions, or contributions, please open an issue in the GitHub repository.
