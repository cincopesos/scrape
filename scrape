#!/usr/bin/env python3
import os
import sys
import psutil
import asyncio
import hashlib
import argparse
from typing import List, Tuple, Dict
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
import re
import requests
import xml.etree.ElementTree as ElementTree
from urllib.parse import urlparse
from collections import defaultdict
from rich.console import Console
from rich.progress import (
    Progress,
    SpinnerColumn,
    TextColumn,
    BarColumn,
    TaskProgressColumn,
)
from rich.panel import Panel
from rich.table import Table
from rich.tree import Tree

# Initialize rich console
console = Console()


async def fetch_url_content(url: str) -> bytes | None:
    """Asynchronously fetches content from a URL."""
    try:
        # Use requests for synchronous fetching within an async context
        # In a real-world high-performance scenario, an async HTTP client like aiohttp or httpx would be better
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, requests.get, url, {"timeout": 10}) # Added timeout
        response.raise_for_status()
        return response.content
    except requests.RequestException as e:
        console.print(f"[yellow]Warning:[/] Failed to fetch {url}: {e}")
        return None


async def get_all_urls_from_sitemap(initial_sitemap_url: str) -> List[str]:
    """
    Recursively fetches and parses sitemaps (XML) to extract all non-sitemap URLs.
    """
    all_final_urls = set()
    sitemaps_to_process = [initial_sitemap_url]
    processed_sitemaps = set()
    namespace = {"ns": "http://www.sitemaps.org/schemas/sitemap/0.9"}

    with console.status("[bold blue]Processing sitemaps...", spinner="dots") as status:
        while sitemaps_to_process:
            current_sitemap_url = sitemaps_to_process.pop(0)
            if current_sitemap_url in processed_sitemaps:
                continue

            status.update(f"[bold blue]Processing sitemap: {current_sitemap_url}[/]")
            processed_sitemaps.add(current_sitemap_url)

            content = await fetch_url_content(current_sitemap_url)
            if not content:
                continue # Skip if fetch failed

            try:
                root = ElementTree.fromstring(content)
                # Check if it's a sitemap index or a URL set
                if root.tag.endswith("sitemapindex"):
                    # Find sitemap locations in a sitemap index
                    sitemap_locs = [loc.text for loc in root.findall(".//ns:loc", namespace)]
                    for loc in sitemap_locs:
                        if loc not in processed_sitemaps and loc not in sitemaps_to_process:
                             sitemaps_to_process.append(loc)
                elif root.tag.endswith("urlset"):
                     # Find URL locations in a URL set
                    url_locs = [loc.text for loc in root.findall(".//ns:loc", namespace)]
                    for loc in url_locs:
                         if loc.lower().endswith(".xml"): # It's another sitemap
                              if loc not in processed_sitemaps and loc not in sitemaps_to_process:
                                   sitemaps_to_process.append(loc)
                         else: # It's a final URL
                              all_final_urls.add(loc)
                else:
                     console.print(f"[yellow]Warning:[/] Unknown root tag '{root.tag}' in {current_sitemap_url}")

            except ElementTree.ParseError as e:
                console.print(f"[red]Error:[/] Failed to parse XML from {current_sitemap_url}: {e}")
            except Exception as e:
                 console.print(f"[red]Error:[/] Unexpected error processing {current_sitemap_url}: {e}")


    return list(all_final_urls)


def get_sitemap_url(site_url):
    # Ensure the base URL has a scheme
    if not site_url.startswith(('http://', 'https://')):
        site_url = 'https://' + site_url # Default to https

    # Remove trailing slash if present for consistency
    if site_url.endswith('/'):
        site_url = site_url[:-1]
    return f"{site_url}/sitemap.xml"


def preview_file_structure(urls: List[str], base_dir: str) -> Tree:
    """Generate a tree preview of the file structure that will be created."""
    file_organizer = FileOrganizer()

    # Create a tree structure for visualization
    tree = Tree(f"[bold blue]{base_dir}[/]")
    dir_trees = {}  # Keep track of directory trees we've created

    # Sort URLs to group directories together
    urls.sort()
    url_count = len(urls)

    # Add total URL count to the root node
    tree.label = f"[bold blue]{base_dir}[/] ([green]{url_count} files[/])"

    for url in urls:
        subdir, filename = file_organizer.organize_path(url)

        if not subdir:
            # Root level files
            tree.add(f"[green]{filename}[/]")
        else:
            # Handle nested directories
            parts = subdir.split(os.sep)
            current_tree = tree
            current_path = ""

            # Create or find each level of the directory tree
            for part in parts:
                current_path = (
                    os.path.join(current_path, part) if current_path else part
                )
                if current_path not in dir_trees:
                    dir_trees[current_path] = current_tree.add(
                        f"[bold yellow]{part}/[/]"
                    )
                current_tree = dir_trees[current_path]

            # Add the file to the deepest directory
            current_tree.add(f"[green]{filename}[/]")

    return tree


class FileOrganizer:
    def __init__(self):
        self.used_names: Dict[str, Dict[str, str]] = defaultdict(dict)

    def clean_filename(self, name: str) -> str:
        """Convert a string into a clean filename."""
        name = re.sub(r"[^\w\-]", " ", name)
        name = re.sub(r"\s+", " ", name)
        name = name.strip().title()
        name = name.replace(" ", "_")
        return name

    def organize_path(self, url: str) -> Tuple[str, str]:
        """
        Organizes the URL into a logical directory structure and filename.
        Returns a tuple of (directory_path, filename).
        """
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split("/") if p]

        if not path_parts:
            return "", "index.md"

        directory_parts = []
        for part in path_parts[:-1]:
            clean_part = self.clean_filename(part)
            if clean_part:
                directory_parts.append(clean_part)

        directory_path = os.path.join(*directory_parts) if directory_parts else ""

        if path_parts:
            filename_base = self.clean_filename(path_parts[-1])
            if not filename_base:
                filename_base = "index"
        else:
            filename_base = "index"

        filename = f"{filename_base}.md"

        if filename in self.used_names[directory_path]:
            if url == self.used_names[directory_path][filename]:
                return directory_path, filename

            counter = 1
            while True:
                new_filename = f"{filename_base}_{counter}.md"
                if new_filename not in self.used_names[directory_path]:
                    filename = new_filename
                    break
                counter += 1

        self.used_names[directory_path][filename] = url

        return directory_path, filename


class MemoryMonitor:
    def __init__(self, verbosity: int = 0):
        self.verbose = verbosity >= 3
        self.peak_memory = 0
        self.process = psutil.Process(os.getpid())

    def log(self, prefix: str = ""):
        if not self.verbose:
            return

        current_mem = self.process.memory_info().rss
        if current_mem > self.peak_memory:
            self.peak_memory = current_mem

        console.print(
            f"[bold cyan]{prefix}[/] Current Memory: [green]{current_mem // (1024 * 1024)} MB[/], "
            f"Peak: [yellow]{self.peak_memory // (1024 * 1024)} MB[/]"
        )

    def get_peak_memory_mb(self) -> int:
        return self.peak_memory // (1024 * 1024)


async def crawl_parallel(
    output_dir: str, urls: List[str], max_concurrent: int = 3, verbosity: int = 0
):
    console.print(Panel("[bold blue]Parallel Scraping[/]"))

    os.makedirs(output_dir, exist_ok=True)
    file_organizer = FileOrganizer()
    memory_monitor = MemoryMonitor(verbosity=verbosity)

    browser_config = BrowserConfig(
        headless=True,
        verbose=verbosity >= 2,
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )
    crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

    with console.status("[bold blue]Starting scraper...[/]") as status:
        crawler = AsyncWebCrawler(config=browser_config)
        await crawler.start()

    try:
        success_count = 0
        fail_count = 0

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=console,
        ) as progress:
            progress_task = progress.add_task(
                "[cyan]Scraping pages...", total=len(urls)
            )

            try:
                for i in range(0, len(urls), max_concurrent):
                    batch = urls[i : i + max_concurrent]
                    tasks = []

                    for j, url in enumerate(batch):
                        session_id = f"parallel_session_{i + j}"
                        crawl_task = crawler.arun(
                            url=url, config=crawl_config, session_id=session_id
                        )
                        tasks.append(crawl_task)

                    try:
                        memory_monitor.log(prefix="→")
                        results = await asyncio.gather(*tasks, return_exceptions=True)
                        memory_monitor.log(prefix="←")

                        for url, result in zip(batch, results):
                            if isinstance(result, Exception):
                                console.print(
                                    f"[red]✗[/] Error scraping [bold]{url}[/]: {result}"
                                )
                                fail_count += 1
                            elif result.success:
                                success_count += 1
                                subdir, filename = file_organizer.organize_path(url)
                                full_dir = os.path.join(output_dir, subdir)
                                os.makedirs(full_dir, exist_ok=True)
                                file_path = os.path.join(full_dir, filename)

                                if verbosity >= 1:
                                    console.print(
                                        f"[green]✓[/] Writing: [bold]{file_path}[/]"
                                    )

                                with open(file_path, "w", encoding="utf-8") as f:
                                    f.write(result.markdown)
                            else:
                                fail_count += 1
                                console.print(
                                    f"[red]✗[/] Failed to scrape [bold]{url}[/]"
                                )

                            progress.advance(progress_task)

                    except Exception as e:
                        console.print(f"[red]✗[/] Batch error: [bold]{str(e)}[/]")
                        fail_count += len(batch)
                        progress.advance(progress_task, len(batch))

            except Exception as e:
                console.print(
                    f"[red]✗[/] Critical error during scraping: [bold]{str(e)}[/]"
                )
                raise

        # Create summary table
        table = Table(title="Scrape Summary", show_header=False)
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="green")
        table.add_row("Successfully scraped", str(success_count))
        table.add_row("Failed", str(fail_count))
        if verbosity >= 3:
            table.add_row(
                "Peak memory usage", f"{memory_monitor.get_peak_memory_mb()} MB"
            )

        console.print("\n")
        console.print(table)

    finally:
        with console.status("[bold blue]Closing scraper...[/]"):
            await crawler.close()


async def main(
    site_url: str, max_concurrent: int, verbosity: int, dry_run: bool = False
):
    console.print(f"\n[bold blue]🌐 Starting scraper for:[/] [bold]{site_url}[/]\n")

    # Determine the initial sitemap URL correctly
    if site_url.lower().endswith('.xml'):
        initial_sitemap_url = site_url
        # Extract base domain for output folder naming if input is a sitemap URL
        parsed_uri = urlparse(site_url)
        # Attempt to get a base domain - might need refinement for complex URLs
        domain_for_folder = parsed_uri.netloc 
    else:
        initial_sitemap_url = get_sitemap_url(site_url)
        domain_for_folder = urlparse(site_url).netloc # Use the provided domain for the folder

    all_urls = await get_all_urls_from_sitemap(initial_sitemap_url)

    if not all_urls:
        console.print(
            "[yellow]⚠️  No URLs found in sitemap(s), scraping the top-level URL only.[/]"
        )
        # Ensure the original URL is properly formatted if used as fallback
        parsed_root = urlparse(site_url)
        root_url = f"{parsed_root.scheme}://{parsed_root.netloc}/"
        urls_to_process = [root_url]
    else:
        console.print(f"[green]✓[/] Found [bold]{len(all_urls)}[/] total URLs in sitemap(s).")
        # Extract unique root URLs
        root_urls = set()
        for url in all_urls:
            try:
                parsed = urlparse(url)
                # Ensure scheme and netloc are present
                if parsed.scheme and parsed.netloc:
                    root_url = f"{parsed.scheme}://{parsed.netloc}/"
                    root_urls.add(root_url)
                else:
                    console.print(f"[yellow]Warning:[/] Skipping invalid URL: {url}")

            except Exception as e:
                 console.print(f"[yellow]Warning:[/] Could not parse URL '{url}': {e}")


        urls_to_process = sorted(list(root_urls)) # Sort for consistent output
        console.print(f"[green]✓[/] Extracted [bold]{len(urls_to_process)}[/] unique root URLs for scraping.")


    # Use the determined domain for the output folder name
    output_folder = os.path.join(os.getcwd(), domain_for_folder)

    if dry_run:
        console.print(
            "\n[bold blue]📁 Preview of file structure for root URLs:[/]"
        )
        # Preview based on the processed root URLs
        tree = preview_file_structure(urls_to_process, domain_for_folder)
        console.print(tree)
        console.print("\n[yellow]This was a dry run. No files were created.[/]")
        return

    os.makedirs(output_folder, exist_ok=True)
    # Crawl the processed root URLs
    await crawl_parallel(
        output_folder, urls_to_process, max_concurrent=max_concurrent, verbosity=verbosity
    )
    console.print("\n[bold green]✨ Scraping complete![/]")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Parallel web scraper with organized output",
        formatter_class=lambda prog: argparse.RawDescriptionHelpFormatter(
            prog, max_help_position=52
        ),
    )
    parser.add_argument(
        "url",
        type=str,
        help="The URL to scrape (must include http:// or https://)",
    )
    parser.add_argument(
        "--max-concurrent",
        type=int,
        default=3,
        help="Maximum number of concurrent crawlers (default: 3)",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="count",
        default=0,
        help="Increase verbosity level (-v for file names, -vv for browser output, -vvv for memory monitoring)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview the file structure without performing the scrape",
    )

    args = parser.parse_args()

    try:
        asyncio.run(main(args.url, args.max_concurrent, args.verbose, args.dry_run))
    except KeyboardInterrupt:
        console.print("\n[bold red]Scraping interrupted by user.[/]")
        sys.exit(1)
    except Exception as e:
        console.print(f"\n[bold red]An error occurred:[/] {str(e)}")
        sys.exit(1)
